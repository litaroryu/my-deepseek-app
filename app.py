import streamlit as st
import requests
import json
from typing import Dict, List, Generator
import re
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="DeepSeek-R1 Chatbot",
    page_icon="ğŸ§ ",
    layout="wide"
)

# Ollama API ì„¤ì •
OLLAMA_BASE_URL = "http://localhost:11434"
DEEPSEEK_MODEL = "deepseek-r1:8b"

class OllamaClient:
    def __init__(self, base_url: str = OLLAMA_BASE_URL):
        self.base_url = base_url

    def check_connection(self) -> bool:
        """Ollama ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False

    def get_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models]
            return []
        except requests.exceptions.RequestException:
            return []

    def chat_stream(self, model: str, messages: List[Dict], **kwargs) -> Generator[str, None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ"""
        payload = {
            "model": model,
            "messages": messages,
            "stream": True,
            **kwargs
        }

        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                stream=True,
                timeout=120  # DeepSeek-R1ì€ ì¶”ë¡  ì‹œê°„ì´ ê¸¸ ìˆ˜ ìˆìŒ
            )

            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line.decode('utf-8'))
                            if 'message' in data and 'content' in data['message']:
                                yield data['message']['content']
                            if data.get('done', False):
                                break
                        except json.JSONDecodeError:
                            continue
            else:
                yield f"ì˜¤ë¥˜ ë°œìƒ: HTTP {response.status_code}"

        except requests.exceptions.RequestException as e:
            yield f"ì—°ê²° ì˜¤ë¥˜: {str(e)}"

def parse_deepseek_response(response: str) -> tuple[str, str]:
    """DeepSeek-R1 ì‘ë‹µì—ì„œ ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ ë¶„ë¦¬"""
    # <think> íƒœê·¸ë¡œ ê°ì‹¸ì§„ ì¶”ë¡  ê³¼ì • ì¶”ì¶œ
    thinking_pattern = r'<think>(.*?)</think>'
    thinking_match = re.search(thinking_pattern, response, re.DOTALL)

    if thinking_match:
        thinking = thinking_match.group(1).strip()
        # <think> íƒœê·¸ ì œê±°í•œ ë‚˜ë¨¸ì§€ê°€ ìµœì¢… ë‹µë³€
        final_answer = re.sub(thinking_pattern, '', response, flags=re.DOTALL).strip()
    else:
        # <think> íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° ì „ì²´ë¥¼ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬
        thinking = ""
        final_answer = response.strip()

    return thinking, final_answer

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

if "ollama_client" not in st.session_state:
    st.session_state.ollama_client = OllamaClient()

if "show_thinking" not in st.session_state:
    st.session_state.show_thinking = True

def main():
    st.title("ğŸ§  DeepSeek-R1 Chatbot")
    st.markdown("**ì¶”ë¡  ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” AI ëª¨ë¸ê³¼ ì±„íŒ…í•˜ê¸°**")

    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        # ì—°ê²° ìƒíƒœ í™•ì¸
        if st.button("ğŸ”— ì—°ê²° í™•ì¸"):
            with st.spinner("Ollama ì„œë²„ ì—°ê²° í™•ì¸ ì¤‘..."):
                is_connected = st.session_state.ollama_client.check_connection()
                if is_connected:
                    st.success("âœ… Ollama ì„œë²„ ì—°ê²°ë¨")

                    # DeepSeek-R1 ëª¨ë¸ í™•ì¸
                    models = st.session_state.ollama_client.get_models()
                    if DEEPSEEK_MODEL in models:
                        st.success(f"âœ… {DEEPSEEK_MODEL} ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
                    else:
                        st.warning(f"âš ï¸ {DEEPSEEK_MODEL} ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
                        st.code(f"ollama pull {DEEPSEEK_MODEL}")
                else:
                    st.error("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        st.divider()

        # DeepSeek-R1 íŠ¹í™” ì„¤ì •
        st.subheader("ğŸ§  DeepSeek-R1 ì˜µì…˜")
        show_thinking = st.checkbox("ğŸ¤” ì¶”ë¡  ê³¼ì • ë³´ê¸°", value=st.session_state.show_thinking)
        st.session_state.show_thinking = show_thinking

        st.info("ğŸ’¡ DeepSeek-R1ì€ ë‹µë³€ ì „ì— ì¶”ë¡  ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”!")

        st.divider()

        # ëª¨ë¸ íŒŒë¼ë¯¸í„°
        st.subheader("ğŸ›ï¸ ìƒì„± ì˜µì…˜")
        temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.1)
        max_tokens = st.slider("Max Tokens", 500, 8000, 4000, 500)
        top_p = st.slider("Top P", 0.1, 1.0, 0.8, 0.1)

        st.divider()

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        st.subheader("ğŸ“ ì§ˆë¬¸ í…œí”Œë¦¿")
        template_options = {
            "ì¼ë°˜ ëŒ€í™”": "",
            "ìˆ˜í•™ ë¬¸ì œ": "ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”:\n",
            "ì½”ë”© ë¬¸ì œ": "ë‹¤ìŒ í”„ë¡œê·¸ë˜ë° ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”:\n",
            "ë…¼ë¦¬ì  ì¶”ë¡ ": "ë‹¤ìŒ ë¬¸ì œë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:\n",
            "ì°½ì˜ì  ê¸€ì“°ê¸°": "ë‹¤ìŒ ì£¼ì œë¡œ ì°½ì˜ì ì¸ ê¸€ì„ ì¨ì£¼ì„¸ìš”:\n"
        }

        selected_template = st.selectbox("ì§ˆë¬¸ ìœ í˜• ì„ íƒ:", list(template_options.keys()))

        st.divider()

        # ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì‚­ì œ"):
            st.session_state.messages = []
            st.rerun()

        # ëŒ€í™” í†µê³„
        if st.session_state.messages:
            st.subheader("ğŸ“Š ëŒ€í™” í†µê³„")
            user_msgs = len([m for m in st.session_state.messages if m["role"] == "user"])
            assistant_msgs = len([m for m in st.session_state.messages if m["role"] == "assistant"])
            st.metric("ì‚¬ìš©ì ë©”ì‹œì§€", user_msgs)
            st.metric("AI ì‘ë‹µ", assistant_msgs)

    # ë©”ì¸ ì±„íŒ… ì˜ì—­
    chat_container = st.container()

    with chat_container:
        # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "assistant" and "thinking" in message:
                    # ì¶”ë¡  ê³¼ì •ì´ ìˆëŠ” ê²½ìš°
                    if st.session_state.show_thinking and message["thinking"]:
                        with st.expander("ğŸ¤” ì¶”ë¡  ê³¼ì • ë³´ê¸°", expanded=False):
                            st.markdown(f"```\n{message['thinking']}\n```")
                    st.markdown(message["content"])
                else:
                    st.markdown(message["content"])

        # ì‚¬ìš©ì ì…ë ¥
        prompt_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")

        if prompt_input:
            # í…œí”Œë¦¿ ì ìš©
            template_prefix = template_options[selected_template]
            full_prompt = template_prefix + prompt_input if template_prefix else prompt_input

            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": full_prompt})

            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            with st.chat_message("user"):
                st.markdown(full_prompt)

            # AI ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                progress_placeholder = st.empty()
                progress_placeholder.info("ğŸ§  DeepSeek-R1ì´ ìƒê°í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

                message_placeholder = st.empty()
                thinking_placeholder = st.empty()
                full_response = ""

                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                try:
                    start_time = time.time()
                    response_stream = st.session_state.ollama_client.chat_stream(
                        model=DEEPSEEK_MODEL,
                        messages=st.session_state.messages,
                        options={
                            "temperature": temperature,
                            "num_predict": max_tokens,
                            "top_p": top_p,
                        }
                    )

                    first_chunk_received = False
                    for chunk in response_stream:
                        if not first_chunk_received:
                            progress_placeholder.empty()  # ì§„í–‰ ìƒí™© ë©”ì‹œì§€ ì œê±°
                            first_chunk_received = True

                        full_response += chunk

                        # ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ ë¶„ë¦¬
                        thinking, final_answer = parse_deepseek_response(full_response)

                        # ì¶”ë¡  ê³¼ì • í‘œì‹œ (ì˜µì…˜ì— ë”°ë¼)
                        if st.session_state.show_thinking and thinking:
                            with thinking_placeholder.expander("ğŸ¤” ì¶”ë¡  ê³¼ì •", expanded=True):
                                st.markdown(f"```\n{thinking}\n```")

                        # ìµœì¢… ë‹µë³€ í‘œì‹œ
                        if final_answer:
                            message_placeholder.markdown(final_answer + "â–Œ")

                    # ìµœì¢… ì •ë¦¬
                    thinking, final_answer = parse_deepseek_response(full_response)

                    if st.session_state.show_thinking and thinking:
                        with thinking_placeholder.expander("ğŸ¤” ì¶”ë¡  ê³¼ì •", expanded=False):
                            st.markdown(f"```\n{thinking}\n```")

                    message_placeholder.markdown(final_answer if final_answer else full_response)

                    # ì‘ë‹µ ì‹œê°„ í‘œì‹œ
                    end_time = time.time()
                    response_time = end_time - start_time
                    st.caption(f"â±ï¸ ì‘ë‹µ ì‹œê°„: {response_time:.1f}ì´ˆ")

                except Exception as e:
                    progress_placeholder.empty()
                    error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                    message_placeholder.error(error_msg)
                    full_response = error_msg
                    thinking = ""
                    final_answer = error_msg

            # ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€ (ì¶”ë¡  ê³¼ì • í¬í•¨)
            thinking, final_answer = parse_deepseek_response(full_response) if not full_response.startswith("ì˜¤ë¥˜") else ("", full_response)
            st.session_state.messages.append({
                "role": "assistant",
                "content": final_answer if final_answer else full_response,
                "thinking": thinking
            })

    # ìƒ˜í”Œ ì§ˆë¬¸ ì œì•ˆ
    st.subheader("ğŸ’¡ ìƒ˜í”Œ ì§ˆë¬¸")
    col1, col2, col3 = st.columns(3)

    sample_questions = [
        "25 Ã— 37ì„ ë‹¨ê³„ë³„ë¡œ ê³„ì‚°í•´ì£¼ì„¸ìš”",
        "Pythonìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•",
        "ì™œ í•˜ëŠ˜ì€ íŒŒë€ìƒ‰ì¼ê¹Œìš”?",
        "ì°½ì˜ì ì¸ ë‹¨í¸ì†Œì„¤ ì•„ì´ë””ì–´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”",
        "ê¸°í›„ë³€í™”ì˜ ì£¼ìš” ì›ì¸ 3ê°€ì§€ëŠ”?",
        "ê°„ë‹¨í•œ ì•”í˜¸í™” ì•Œê³ ë¦¬ì¦˜ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    ]

    for i, question in enumerate(sample_questions):
        col = [col1, col2, col3][i % 3]
        if col.button(f"â“ {question[:20]}...", key=f"sample_{i}"):
            st.session_state.sample_question = question
            st.rerun()

    # ìƒ˜í”Œ ì§ˆë¬¸ ìë™ ì…ë ¥
    if hasattr(st.session_state, 'sample_question'):
        st.session_state.messages.append({"role": "user", "content": st.session_state.sample_question})
        del st.session_state.sample_question
        st.rerun()

    # í•˜ë‹¨ ì •ë³´
    with st.expander("â„¹ï¸ DeepSeek-R1 ì‚¬ìš© ê°€ì´ë“œ"):
        st.markdown("""
        ### ğŸ§  DeepSeek-R1 ëª¨ë¸ íŠ¹ì§•:
        - **ì¶”ë¡  ê¸°ë°˜**: ë‹µë³€í•˜ê¸° ì „ì— ë‹¨ê³„ë³„ ì‚¬ê³  ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤
        - **íˆ¬ëª…ì„±**: ì–´ë–»ê²Œ ê²°ë¡ ì— ë„ë‹¬í–ˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤
        - **ì •í™•ì„±**: ë³µì¡í•œ ë¬¸ì œë„ ì²´ê³„ì ìœ¼ë¡œ í•´ê²°í•©ë‹ˆë‹¤

        ### ğŸ“¥ ì„¤ì¹˜ ë°©ë²•:
        ```bash
        # DeepSeek-R1 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì•½ 4.7GB)
        ollama pull deepseek-r1:8b

        # Ollama ì„œë²„ ì‹¤í–‰
        ollama serve
        ```

        ### ğŸ’¡ í™œìš© íŒ:
        - **ìˆ˜í•™ ë¬¸ì œ**: ë‹¨ê³„ë³„ ê³„ì‚° ê³¼ì •ì„ ëª…í™•íˆ ë³´ì—¬ì¤ë‹ˆë‹¤
        - **ì½”ë”© ë¬¸ì œ**: ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ë¶€í„° êµ¬í˜„ê¹Œì§€ ì„¤ëª…í•©ë‹ˆë‹¤
        - **ë…¼ë¦¬ ì¶”ë¡ **: ë³µì¡í•œ ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤
        - **ì°½ì˜ì  ì‘ì—…**: ì•„ì´ë””ì–´ ë°œì „ ê³¼ì •ì„ íˆ¬ëª…í•˜ê²Œ ê³µê°œí•©ë‹ˆë‹¤

        ### âš™ï¸ ìµœì  ì„¤ì •:
        - Temperature: 0.1-0.5 (ë…¼ë¦¬ì  ë‹µë³€) / 0.6-0.8 (ì°½ì˜ì  ë‹µë³€)
        - Max Tokens: 4000+ (ì¶©ë¶„í•œ ì¶”ë¡  ê³µê°„ ì œê³µ)
        """)

if __name__ == "__main__":
    main()